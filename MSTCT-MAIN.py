# -*- coding: utf-8 -*-
"""Copy of web-ui.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IfDJjycOXuyrrJPYwLx-NXuSYleFfc8b
"""

!pip install -U streamlit pyngrok

import getpass
import os

if not os.environ.get("Ngrok_API_KEY"):
    os.environ["Ngrok_API_KEY"] = getpass.getpass("Enter API key for ngrok:\n")

import torch
print(torch.__version__)

!git clone https://github.com/piergiaj/pytorch-i3d
!git clone https://github.com/dairui01/MS-TCT.git /content/MS-TCT
!gdown 1o2OuunXp-qBUSS-HloqH1QEEmLcT0fdZ
!gdown 1kNnH1fUOueazdcd3iJJnCacnTmOYbwze

#feature_extraction.py
import os
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from tqdm import tqdm
import glob
import json

# Import I3D model
import sys
sys.path.append('/content/pytorch-i3d')
from pytorch_i3d import InceptionI3d

class CharadesVideoLoader:
    """Load video frames for MS-TCT compatible feature extraction"""
    def __init__(self, video_path, snippet_length=16, target_fps=8):
        """
        Args:
            video_path: Path to video file
            snippet_length: Number of frames per I3D snippet (16)
            target_fps: Target FPS for sampling (8 fps from 24fps video)
        """
        self.video_path = video_path
        self.snippet_length = snippet_length
        self.target_fps = target_fps

    def load_video_frames(self):
        """Load and sample video frames at target FPS"""
        cap = cv2.VideoCapture(self.video_path)

        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {self.video_path}")

        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Calculate sampling rate: sample every Nth frame
        sample_every = int(round(video_fps / self.target_fps))

        frames = []
        frame_idx = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # Sample frames at target FPS
            if frame_idx % sample_every == 0:
                # Convert BGR to RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # Resize to 224x224 for I3D
                frame = cv2.resize(frame, (224, 224))
                frames.append(frame)

            frame_idx += 1

        cap.release()

        if len(frames) == 0:
            raise ValueError(f"No frames extracted from {self.video_path}")

        return np.array(frames), video_fps, total_frames

    def preprocess_frames(self, frames):
        """Normalize frames for I3D"""
        # Convert to float and normalize to [-1, 1]
        frames = frames.astype(np.float32) / 255.0
        frames = (frames - 0.5) * 2.0
        return frames

    def get_sliding_window_snippets(self, frames):
        """
        Extract snippets using sliding window approach
        Each snippet is snippet_length frames
        Returns: list of snippets [snippet_length, H, W, C]
        """
        snippets = []
        num_frames = len(frames)

        # Create one snippet per frame (centered on that frame)
        for center_idx in range(num_frames):
            # Calculate start and end indices for this snippet
            start_idx = max(0, center_idx - self.snippet_length // 2)
            end_idx = start_idx + self.snippet_length

            # Adjust if we go past the end
            if end_idx > num_frames:
                end_idx = num_frames
                start_idx = max(0, end_idx - self.snippet_length)

            # Extract snippet
            snippet = frames[start_idx:end_idx]

            # Pad if needed (at beginning or end of video)
            if len(snippet) < self.snippet_length:
                pad_before = max(0, self.snippet_length // 2 - center_idx)
                pad_after = self.snippet_length - len(snippet) - pad_before

                if pad_before > 0:
                    snippet = np.concatenate([snippet[:1].repeat(pad_before, axis=0), snippet], axis=0)
                if pad_after > 0:
                    snippet = np.concatenate([snippet, snippet[-1:].repeat(pad_after, axis=0)], axis=0)

            snippets.append(snippet)

        return snippets


class I3DFeatureExtractor:
    """Extract I3D features compatible with MS-TCT (400-dim per snippet)"""
    def __init__(self, model_path, device='cuda'):
        self.device = device if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")

        # Load I3D model with 400 output classes
        self.model = InceptionI3d(400, in_channels=3)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()

    def extract_snippet_features(self, snippet):
        """
        Extract 400-dim features from a single snippet
        Args:
            snippet: numpy array of shape [16, H, W, C]
        Returns:
            Feature vector of shape [400]
        """
        # Convert to tensor: [T, H, W, C] -> [1, C, T, H, W]
        snippet_tensor = torch.from_numpy(snippet).permute(3, 0, 1, 2).unsqueeze(0)
        snippet_tensor = snippet_tensor.to(self.device).float()

        with torch.no_grad():
            # Forward pass - get logits (class predictions)
            logits = self.model(snippet_tensor)

            # The output should be [1, 400, T, H, W] or similar
            # We need to reduce to [400] by averaging over spatial and temporal dims

            if logits.dim() == 5:  # [B, C, T, H, W]
                # Average over time, height, width
                features = logits.mean(dim=[2, 3, 4]).squeeze(0)  # [400]
            elif logits.dim() == 3:  # [B, C, T]
                # Average over time
                features = logits.mean(dim=2).squeeze(0)  # [400]
            elif logits.dim() == 2:  # [B, C]
                features = logits.squeeze(0)  # [400]
            else:
                # Fallback: squeeze all batch dims
                features = logits.squeeze()
                if features.dim() > 1:
                    features = features.mean(dim=list(range(1, features.dim())))

            # Ensure we have exactly 400 dimensions
            features_np = features.cpu().numpy()

            if features_np.shape[0] != 400:
                raise ValueError(f"Expected 400-dim features, got {features_np.shape[0]}")

            return features_np

    def extract_video_features(self, video_path, snippet_length=16, target_fps=8):
        """
        Extract per-frame features for entire video

        Args:
            video_path: Path to video
            snippet_length: Frames per snippet (16)
            target_fps: Target sampling FPS (8 for Charades)

        Returns:
            features: numpy array of shape [num_frames, 400]
            metadata: dict with video info
        """
        try:
            # Load video
            loader = CharadesVideoLoader(
                video_path,
                snippet_length=snippet_length,
                target_fps=target_fps
            )

            frames, video_fps, total_frames = loader.load_video_frames()
            frames = loader.preprocess_frames(frames)
            snippets = loader.get_sliding_window_snippets(frames)

            # Extract features for each snippet (one per frame)
            features_list = []
            for snippet in snippets:
                feat = self.extract_snippet_features(snippet)
                features_list.append(feat)

            # Stack into array [num_frames, 400]
            features = np.stack(features_list, axis=0)

            metadata = {
                'video_fps': video_fps,
                'total_frames': total_frames,
                'sampled_frames': len(frames),
                'num_features': len(snippets),
                'target_fps': target_fps,
                'snippet_length': snippet_length,
                'feature_dim': 400
            }

            return features, metadata

        except Exception as e:
            print(f"Error processing {video_path}: {str(e)}")
            import traceback
            traceback.print_exc()
            return None, None


def extract_features_for_mstct(video_dir, output_dir, model_path,
                                annotation_json, num_videos=None,
                                snippet_length=16, target_fps=8):
    """
    Extract I3D features compatible with MS-TCT
    Output: [num_frames, 400] per video

    Args:
        video_dir: Directory containing videos
        output_dir: Where to save features
        model_path: Path to I3D model weights
        annotation_json: Path to charades json
        num_videos: Number to process (None = all)
        snippet_length: I3D snippet length (16)
        target_fps: Sampling rate (8 fps)
    """

    os.makedirs(output_dir, exist_ok=True)

    # Load annotations
    with open(annotation_json, 'r') as f:
        annotations = json.load(f)

    video_ids = list(annotations.keys())
    if num_videos is not None:
        video_ids = video_ids[:num_videos]

    print(f"Processing {len(video_ids)} videos")
    print(f"Settings: snippet_length={snippet_length}, target_fps={target_fps}")
    print(f"Expected output: [num_frames, 400] per video")
    print()

    # Initialize extractor
    extractor = I3DFeatureExtractor(model_path)

    # Process each video
    successful = 0
    failed = []

    for video_id in tqdm(video_ids, desc="Extracting features"):
        video_path = os.path.join(video_dir, f"{video_id}.mp4")
        output_path = os.path.join(output_dir, f"{video_id}.npy")

        # Skip if doesn't exist
        if not os.path.exists(video_path):
            print(f"Video not found: {video_path}")
            failed.append(video_id)
            continue

        # Skip if already processed
        if os.path.exists(output_path):
            # Verify it's correct shape
            existing = np.load(output_path)
            if existing.shape[1] == 400:
                successful += 1
                continue
            else:
                print(f"Re-extracting {video_id} (wrong shape: {existing.shape})")

        # Extract features
        features, metadata = extractor.extract_video_features(
            video_path,
            snippet_length=snippet_length,
            target_fps=target_fps
        )

        if features is not None:
            # Verify shape
            assert features.shape[1] == 400, f"Wrong feature dim: {features.shape}"

            # Save features
            np.save(output_path, features)

            # Check against annotation
            anno = annotations[video_id]
            expected_frames = int(anno['duration'] * target_fps)

            print(f"✓ {video_id}: shape {features.shape}, " +
                  f"expected ~{expected_frames} frames @ {target_fps}fps")
            successful += 1
        else:
            failed.append(video_id)

    # Summary
    print("\n" + "="*60)
    print(f"Feature Extraction Complete!")
    print(f"Successful: {successful}/{len(video_ids)}")
    if failed:
        print(f"Failed: {len(failed)}")
        print(f"Failed videos: {', '.join(failed[:10])}")
    print("="*60)

    # Verify a sample
    if successful > 0:
        sample_file = os.path.join(output_dir, f"{video_ids[0]}.npy")
        if os.path.exists(sample_file):
            sample = np.load(sample_file)
            print(f"\nSample verification:")
            print(f"  File: {video_ids[0]}.npy")
            print(f"  Shape: {sample.shape}")
            print(f"  Expected: [num_frames, 400]")
            print(f"  ✓ Correct!" if sample.shape[1] == 400 else f"  ✗ WRONG!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app_.py
# """
# MS-TCT Real-Time Inference
# Given an MP4 video path, returns temporal action predictions
# """
# 
# import os
# import numpy as np
# import torch
# import cv2
# import json
# from torch.autograd import Variable
# import torch.nn.functional as F
# import matplotlib.pyplot as plt
# from matplotlib.patches import Rectangle
# import sys
# 
# # Add paths
# sys.path.append('/content/pytorch-i3d')
# sys.path.append('/content/MS-TCT')
# 
# from pytorch_i3d import InceptionI3d
# 
# 
# class VideoActionDetector:
#     """Real-time action detection for videos"""
# 
#     def __init__(self, model_path, i3d_weights_path, class_names_path=None):
#         """
#         Initialize the detector
# 
#         Args:
#             model_path: Path to trained MS-TCT model checkpoint (.pkl)
#             i3d_weights_path: Path to I3D weights for feature extraction
#             class_names_path: Path to Charades class names file
#         """
#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
#         print(f"Using device: {self.device}")
# 
#         # Load I3D for feature extraction
#         print("Loading I3D model...")
#         self.i3d_model = InceptionI3d(400, in_channels=3)
#         self.i3d_model.load_state_dict(torch.load(i3d_weights_path, map_location=self.device))
#         self.i3d_model.to(self.device)
#         self.i3d_model.eval()
# 
#         # Load MS-TCT model
#         print("Loading MS-TCT model...")
#         from MSTCT.MSTCT_Model import MSTCT
# 
#         # Model parameters (same as training)
#         inter_channels = [256, 384, 576, 864]
#         num_block = 3
#         head = 8
#         mlp_ratio = 8
#         in_feat_dim = 400
#         final_embedding_dim = 512
#         num_classes = 157
# 
#         self.mstct_model = MSTCT(
#             inter_channels, num_block, head, mlp_ratio,
#             in_feat_dim, final_embedding_dim, num_classes
#         )
#         self.mstct_model.to(self.device)
#         self.mstct_model.eval()
# 
#         # Load trained weights
#         if os.path.exists(model_path):
#             import pickle
#             with open(model_path, 'rb') as f:
#                 saved_probs = pickle.load(f)
#             print(f"✓ Loaded predictions from {model_path}")
#             # Note: The .pkl file contains predictions, not model weights
#             # For actual inference, we'd need to load model weights (.pth)
#             # For now, we'll use the model in eval mode
# 
#         # Load class names
#         self.class_names = self._load_class_names(class_names_path)
# 
#         print("✓ Detector ready!")
# 
#     def _load_class_names(self, class_names_path):
#         """Load Charades action class names"""
#         if class_names_path and os.path.exists(class_names_path):
#             classes = {}
#             with open(class_names_path, 'r') as f:
#                 for line in f:
#                     parts = line.strip().split()
#                     if len(parts) >= 2:
#                         class_id = parts[0]
#                         class_name = ' '.join(parts[1:])
#                         classes[int(class_id[1:])] = class_name
#             return classes
#         else:
#             # Default class IDs
#             return {i: f"Action_{i:03d}" for i in range(157)}
# 
#     def extract_features(self, video_path, target_fps=8, snippet_length=16):
#         """
#         Extract I3D features from video
# 
#         Args:
#             video_path: Path to video file
#             target_fps: Target sampling FPS
#             snippet_length: Frames per I3D snippet
# 
#         Returns:
#             features: numpy array [num_frames, 400]
#             video_info: dict with metadata
#         """
#         cap = cv2.VideoCapture(video_path)
# 
#         if not cap.isOpened():
#             raise ValueError(f"Cannot open video: {video_path}")
# 
#         video_fps = cap.get(cv2.CAP_PROP_FPS)
#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         duration = total_frames / video_fps
# 
#         # Sample frames at target FPS
#         sample_every = int(round(video_fps / target_fps))
# 
#         frames = []
#         frame_idx = 0
# 
#         print(f"Reading video: {os.path.basename(video_path)}")
#         print(f"  Duration: {duration:.2f}s, FPS: {video_fps:.2f}, Total frames: {total_frames}")
#         print(f"  Sampling every {sample_every} frames (target {target_fps} fps)")
# 
#         while True:
#             ret, frame = cap.read()
#             if not ret:
#                 break
# 
#             if frame_idx % sample_every == 0:
#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#                 frame = cv2.resize(frame, (224, 224))
#                 frames.append(frame)
# 
#             frame_idx += 1
# 
#         cap.release()
# 
#         if len(frames) == 0:
#             raise ValueError("No frames extracted from video")
# 
#         print(f"  Extracted {len(frames)} frames @ {target_fps} fps")
# 
#         # Preprocess frames
#         frames = np.array(frames).astype(np.float32) / 255.0
#         frames = (frames - 0.5) * 2.0
# 
#         # Extract features using I3D
#         features_list = []
# 
#         print("  Extracting I3D features...")
#         with torch.no_grad():
#             for center_idx in range(len(frames)):
#                 # Create snippet centered on current frame
#                 start_idx = max(0, center_idx - snippet_length // 2)
#                 end_idx = start_idx + snippet_length
# 
#                 if end_idx > len(frames):
#                     end_idx = len(frames)
#                     start_idx = max(0, end_idx - snippet_length)
# 
#                 snippet = frames[start_idx:end_idx]
# 
#                 # Pad if needed
#                 if len(snippet) < snippet_length:
#                     pad_before = max(0, snippet_length // 2 - center_idx)
#                     pad_after = snippet_length - len(snippet) - pad_before
# 
#                     if pad_before > 0:
#                         snippet = np.concatenate([snippet[:1].repeat(pad_before, axis=0), snippet], axis=0)
#                     if pad_after > 0:
#                         snippet = np.concatenate([snippet, snippet[-1:].repeat(pad_after, axis=0)], axis=0)
# 
#                 # Convert to tensor [1, C, T, H, W]
#                 snippet_tensor = torch.from_numpy(snippet).permute(3, 0, 1, 2).unsqueeze(0)
#                 snippet_tensor = snippet_tensor.to(self.device).float()
# 
#                 # Extract features
#                 logits = self.i3d_model(snippet_tensor)
# 
#                 if logits.dim() == 5:
#                     features = logits.mean(dim=[2, 3, 4]).squeeze(0)
#                 elif logits.dim() == 3:
#                     features = logits.mean(dim=2).squeeze(0)
#                 else:
#                     features = logits.squeeze()
# 
#                 features_list.append(features.cpu().numpy())
# 
#         features = np.stack(features_list, axis=0)
# 
#         video_info = {
#             'duration': duration,
#             'fps': video_fps,
#             'total_frames': total_frames,
#             'sampled_frames': len(frames),
#             'target_fps': target_fps,
#             'features_shape': features.shape
#         }
# 
#         print(f"  ✓ Features extracted: {features.shape}")
# 
#         return features, video_info
# 
#     def predict(self, video_path, threshold=0.5, top_k=5):
#         """
#         Predict actions in video
# 
#         Args:
#             video_path: Path to video file
#             threshold: Confidence threshold for predictions
#             top_k: Return top K most confident actions per frame
# 
#         Returns:
#             predictions: dict with temporal predictions
#         """
#         # Extract features
#         features, video_info = self.extract_features(video_path)
# 
#         # Prepare features for MS-TCT
#         # Model expects [batch, channels, temporal] = [1, 400, T]
#         # Conv1d operates on the temporal dimension with 400 input channels
#         features_tensor = torch.from_numpy(features).unsqueeze(0)  # [1, T, 400]
#         features_tensor = features_tensor.permute(0, 2, 1)  # [1, 400, T]
#         features_tensor = features_tensor.to(self.device).float()
# 
#         # Forward pass through MS-TCT
#         print("  Running MS-TCT inference...")
#         with torch.no_grad():
#             print(f"  Input shape: {features_tensor.shape}")  # Should be [1, 400, 199]
#             outputs, heatmap = self.mstct_model(features_tensor)
#             # outputs shape: [1, T, 157]
#             probs = torch.sigmoid(outputs).cpu().numpy()[0]  # [T, 157]
# 
#         print(f"  ✓ Predictions generated: {probs.shape}")
# 
#         # Process predictions
#         predictions = self._process_predictions(
#             probs, video_info, threshold, top_k
#         )
# 
#         return predictions
# 
#     def _process_predictions(self, probs, video_info, threshold, top_k):
#         """Process raw predictions into structured format"""
# 
#         num_frames = probs.shape[0]
#         fps = video_info['target_fps']
#         duration = video_info['duration']
# 
#         # Find detections
#         detections = []
# 
#         for frame_idx in range(num_frames):
#             frame_probs = probs[frame_idx]
#             timestamp = frame_idx / fps
# 
#             # Get top-k predictions for this frame
#             top_indices = np.argsort(frame_probs)[-top_k:][::-1]
#             top_probs = frame_probs[top_indices]
# 
#             # Filter by threshold
#             valid_mask = top_probs >= threshold
# 
#             if np.any(valid_mask):
#                 for class_idx, prob in zip(top_indices[valid_mask], top_probs[valid_mask]):
#                     detections.append({
#                         'frame': frame_idx,
#                         'timestamp': timestamp,
#                         'class_id': int(class_idx),
#                         'class_name': self.class_names[class_idx],
#                         'confidence': float(prob)
#                     })
# 
#         # Aggregate into temporal segments
#         segments = self._aggregate_segments(detections, fps)
# 
#         return {
#             'video_info': video_info,
#             'detections': detections,
#             'segments': segments,
#             'frame_predictions': probs
#         }
# 
#     def _aggregate_segments(self, detections, fps, gap_threshold=1.0):
#         """Aggregate frame-level detections into temporal segments"""
# 
#         if not detections:
#             return []
# 
#         # Group by class
#         by_class = {}
#         for det in detections:
#             class_id = det['class_id']
#             if class_id not in by_class:
#                 by_class[class_id] = []
#             by_class[class_id].append(det)
# 
#         # Create segments
#         segments = []
# 
#         for class_id, dets in by_class.items():
#             # Sort by timestamp
#             dets = sorted(dets, key=lambda x: x['timestamp'])
# 
#             # Merge nearby detections
#             current_segment = None
# 
#             for det in dets:
#                 if current_segment is None:
#                     current_segment = {
#                         'class_id': class_id,
#                         'class_name': det['class_name'],
#                         'start_time': det['timestamp'],
#                         'end_time': det['timestamp'],
#                         'confidences': [det['confidence']],
#                         'frames': [det['frame']]
#                     }
#                 elif det['timestamp'] - current_segment['end_time'] <= gap_threshold:
#                     # Extend current segment
#                     current_segment['end_time'] = det['timestamp']
#                     current_segment['confidences'].append(det['confidence'])
#                     current_segment['frames'].append(det['frame'])
#                 else:
#                     # Save current segment and start new one
#                     current_segment['avg_confidence'] = np.mean(current_segment['confidences'])
#                     current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
#                     segments.append(current_segment)
# 
#                     current_segment = {
#                         'class_id': class_id,
#                         'class_name': det['class_name'],
#                         'start_time': det['timestamp'],
#                         'end_time': det['timestamp'],
#                         'confidences': [det['confidence']],
#                         'frames': [det['frame']]
#                     }
# 
#             # Add last segment
#             if current_segment:
#                 current_segment['avg_confidence'] = np.mean(current_segment['confidences'])
#                 current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
#                 segments.append(current_segment)
# 
#         # Sort by confidence
#         segments = sorted(segments, key=lambda x: x['avg_confidence'], reverse=True)
# 
#         return segments
# 
#     def visualize_predictions(self, predictions, save_path=None):
#         """Visualize temporal predictions"""
# 
#         segments = predictions['segments']
#         duration = predictions['video_info']['duration']
# 
#         if not segments:
#             print("No predictions above threshold")
#             return
# 
#         # Create timeline visualization
#         fig, ax = plt.subplots(figsize=(15, 8))
# 
#         # Plot segments
#         colors = plt.cm.tab20(np.linspace(0, 1, 20))
#         class_to_color = {}
#         y_pos = 0
# 
#         for seg in segments[:20]:  # Show top 20
#             class_id = seg['class_id']
#             if class_id not in class_to_color:
#                 class_to_color[class_id] = colors[len(class_to_color) % 20]
# 
#             color = class_to_color[class_id]
# 
#             rect = Rectangle(
#                 (seg['start_time'], y_pos),
#                 seg['duration'],
#                 0.8,
#                 facecolor=color,
#                 alpha=seg['avg_confidence'],
#                 edgecolor='black',
#                 linewidth=1
#             )
#             ax.add_patch(rect)
# 
#             # Add label
#             label = f"{seg['class_name'][:30]} ({seg['avg_confidence']:.2f})"
#             ax.text(
#                 seg['start_time'] + seg['duration']/2,
#                 y_pos + 0.4,
#                 label,
#                 ha='center',
#                 va='center',
#                 fontsize=8,
#                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.7)
#             )
# 
#             y_pos += 1
# 
#         ax.set_xlim(0, duration)
#         ax.set_ylim(-0.5, y_pos)
#         ax.set_xlabel('Time (seconds)', fontsize=12)
#         ax.set_ylabel('Actions', fontsize=12)
#         ax.set_title('Temporal Action Detection Results', fontsize=14, fontweight='bold')
#         ax.grid(True, alpha=0.3)
# 
#         plt.tight_layout()
# 
#         if save_path:
#             plt.savefig(save_path, dpi=150, bbox_inches='tight')
#             print(f"✓ Saved visualization to {save_path}")
# 
#         plt.show()
# 
#     def print_predictions(self, predictions, max_segments=10):
#         """Print predictions in readable format"""
# 
#         segments = predictions['segments']
#         video_info = predictions['video_info']
# 
#         print("\n" + "="*70)
#         print("VIDEO ACTION DETECTION RESULTS")
#         print("="*70)
#         print(f"Video duration: {video_info['duration']:.2f}s")
#         print(f"Total detections: {len(predictions['detections'])}")
#         print(f"Unique actions: {len(segments)}")
#         print("="*70)
# 
#         if not segments:
#             print("No actions detected above threshold")
#             return
# 
#         print(f"\nTop {min(max_segments, len(segments))} Action Segments:")
#         print("-"*70)
# 
#         for i, seg in enumerate(segments[:max_segments], 1):
#             print(f"\n{i}. {seg['class_name']}")
#             print(f"   Time: {seg['start_time']:.2f}s - {seg['end_time']:.2f}s (duration: {seg['duration']:.2f}s)")
#             print(f"   Confidence: {seg['avg_confidence']:.2%}")
#             print(f"   Frames: {len(seg['frames'])}")
#

from pyngrok import ngrok

ngrok_key = os.environ.get("Ngrok_API_KEY")
port = 8501

ngrok.set_auth_token(ngrok_key)
ngrok.connect(port).public_url

!rm -rf logs.txt && streamlit run app.py &>/content/logs.txt

